
\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}

\title{High Capacity Neural Network}

\date{2016-10-06}

\author{Domenic Donato}

\begin{document}

\pagenumbering{gobble} 
\maketitle
\newpage
\pagenumbering{arabic} 

\section{Introduction}

TODO(domenic): Write this section

\section{Related Literature}

TODO(domenic): Write this section

\subsection{Forward Computation}

The forward computation of a fully connected feedforward neural network with $N$ layers can be described as follows:

\begin{equation}
x_{n-1} = a_n(t_n(x_n))
\end{equation}

$\{N \in \mathbb{N} | N > 0\}$

${n \in N}$

$x_n$ is a vector

$x_N$ is the input vector

$\hat{y} = x_1$ is the output vector

$t_n(x)$ is a transform function

$a_n(x)$ is an activation function\\

The standard transform function, $t_n(x)$, is:

\begin{equation} \label{eq:standard_transform}
t_n(x) = f_n(x)
\end{equation}

\begin{equation} \label{eq:f}
f_n(x) = x \cdot W_n + b_n
\end{equation}

$W_n \in \mathbb{R}^2$

$b_n \in \mathbb{R}$\\

The transform function proposed in this paper is:

\begin{equation} \label{eq:quad_transform}
t_n(x) =  f_n(x) \cdot g(x)
\end{equation}

\begin{equation} \label{eq:g}
g_n(x) = x \cdot Y_n + c_n
\end{equation}

$Y_n \in \mathbb{R}^2$

$c_n \in \mathbb{R}$\\

Rather than using the standard transform, \eqref{eq:standard_transform}, a quadratic transform, \eqref{eq:quad_transform}, is performed. The quadratic transform is a superset of the standard design, which becomes apparent when $Y_n \in \{0,...,0\}$ and $c_n \in \{1, ..., 1\}$. Using a quadratic transformation enables the neural network to represent any bounded degree polynomial over an infinite domain using a finite number of nodes. The standard design would require an infinite number of nodes to do the same.

\subsection{Backward Computation}

The total network error given a cost function $c(y, \hat{y})$ is:

\begin{equation} \label{eq:total_error}
E = \sum_i c(y_i, \hat{y_i})
\end{equation}

A squared error cost function was used for the experiments in this paper.

\begin{equation} \label{eq:squared_cost}
c(y, \hat{y}) = \frac{(y - \hat{y})^2}{2}
\end{equation}

\begin{equation} \label{eq:squared_cost_prime}
c'(y, \hat{y}) = y - \hat{y}
\end{equation}

Back propagation was used to train the networks. The key thing about back propagation is that its a dynamic program and caches part of the gradient calculation so it can be reused upstream.\\

$C_n$ is the matrix of cached calculations at level $n$\\

The cache is seeded with the derivative of the cost function: 

\begin{equation}
C_1 = c'(y, \hat{y})
\end{equation}

At each layer the cache is updated as follows:

\begin{equation}
C_{n+1} = a'_n(x_n) \cdot (C_n \cdot t'_n(x_n)^\top)
\end{equation}

While visiting a layer, the gradients for each parameter in $\theta_n$ is calculated:

\begin{equation}
\delta\theta_n = \theta'_n \cdot C_n
\end{equation}

\section{Experiments}

TODO(domenic): Write this section

\section{Conclusion}

TODO(domenic): Write this section

\end{document}
